# -*- coding: utf-8 -*-
"""WebScraping_plusieurs_pages_Antonin.ipynb

Automatically generated by Colab.

"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import seaborn as sns

# Observe attentivement l'URL de la page 1. Essaye de passer à la page 2. Dans un notebook python,
# essaye de générer automatiquement les URL des 5 premières pages (avec une boucle par exemple ?)

# recherche pour transformer la str en num et supprimer () / 10 pour ne conserver que le float afin de créer ma data viz

# utiliser regex, créer fonction pour appliquer à l'ensemble du df avec un lambda
import re

url = f"http://www.chucknorrisfacts.fr/facts/history/1"
response = requests.get(url)
html = BeautifulSoup(response.text, "html.parser")
blague_note = html.find_all("span", id_ = "")
#note_num = blague_note[7].get_text()

note_num = float(re.sub(r"[()/]|10", "", blague_note[7].get_text()))
note_num



page = 1
blagues_global = []
notes_global = []
while page < 6:
  url = f"http://www.chucknorrisfacts.fr/facts/history/{page}"
  response = requests.get(url)
  html = BeautifulSoup(response.text, "html.parser")
  blague_bloc = html.find_all(class_ = "card")
  blague_page = [item.find(class_ = "card-text").get_text() for item in blague_bloc]
  # Extraction des notes
  note_page = [
        float(re.sub(r"[()/]|10", "", item.find("span", id_ ="").get_text()))
        for item in blague_bloc
        if item.find("span")  # Vérifie que l'élément existe
            ]
  blagues_global.extend(blague_page)
  notes_global.extend(note_page)
  page += 1
  dico_norris = {
    "blague" : blagues_global,
    "note" : notes_global,
               }


  df_norris = pd.DataFrame(dico_norris)

df_norris

sns.displot(df_norris, x="note", bins = 10)

"""#Code commenté"""

# Initialisation de la variable `page` pour commencer à récupérer les blagues depuis la première page
page = 1

# Création de deux listes vides pour stocker les blagues et leurs notes respectives
blagues = []
notes = []

# Une boucle `while` qui s'exécute tant que `page` est inférieure à 6 (pour collecter des données sur 5 pages)
while page < 6:
    # Construction de l'URL cible en y insérant le numéro de page dynamique
    url = f"http://www.chucknorrisfacts.fr/facts/history/{page}"

    # Envoi d'une requête GET à l'URL pour récupérer le contenu HTML de la page
    response = requests.get(url)

    # Utilisation de BeautifulSoup pour analyser le HTML récupéré
    html = BeautifulSoup(response.text, "html.parser")

    # Recherche de tous les blocs HTML contenant les blagues (identifiés par leur classe "card")
    blague_bloc = html.find_all(class_="card")

    # Extraction du texte des blagues à partir de chaque bloc trouvé
    blague = [item.find(class_="card-text").get_text() for item in blague_bloc] # liste temporaire pour une page

    # Extraction des notes associées aux blagues
    note = [
        # Conversion de la note en texte (ex. "(8/10)") en un nombre flottant (-> 8.0)
        float(re.sub(r"[()/]|10", "", item.find("span", id_="").get_text()))
        for item in blague_bloc
        if item.find("span")  # Vérifie que l'élément "span" contenant la note existe
    ]   # liste temporaire pour une page

    # Ajout des blagues extraites à la liste globale `blagues`
    blagues.extend(blague)

    # Ajout des notes extraites à la liste globale `notes`
    notes.extend(note)

    # Passage à la page suivante
    page += 1

# Création d'un dictionnaire associant chaque blague à sa note
dico_norris = {
    "blague": blagues,
    "note": notes,
}

df_norris = pd.DataFrame(dico_norris)
df_norris
